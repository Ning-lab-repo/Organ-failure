{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, recall_score, confusion_matrix, precision_score, f1_score, matthews_corrcoef\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Load test data from CSV file\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Ensure the column order of X_test matches that of the training data (X)\n",
    "X_test = test_data[X.columns]\n",
    "y_test = test_data['heart']\n",
    "\n",
    "# List of trained models to evaluate\n",
    "models = [lgr, svc, rf, mlp, gbdt, ada, xgb, lgbm, catboost]\n",
    "\n",
    "# Create an empty list to store results for each model\n",
    "results = []\n",
    "\n",
    "plt.figure(figsize=(10, 8))  # Create a new figure for plotting\n",
    "\n",
    "# Loop through each model to calculate metrics and plot ROC curve\n",
    "for model in models:\n",
    "    # Check if the model has a decision function (used for SVM, etc.)\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        y_score = model.decision_function(X_test)\n",
    "    else:\n",
    "        # Otherwise, use predicted probabilities (e.g., for random forest, logistic regression)\n",
    "        y_score = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate ROC curve parameters (FPR: false positive rate, TPR: true positive rate)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    auc = roc_auc_score(y_test, y_score)  # Calculate AUC (Area Under Curve)\n",
    "    \n",
    "    # Calculate other evaluation metrics\n",
    "    y_pred = model.predict(X_test)  # Get model predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Accuracy\n",
    "    sensitivity = recall_score(y_test, y_pred)  # Sensitivity (Recall)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()  # Confusion matrix\n",
    "    specificity = tn / (tn + fp)  # Specificity\n",
    "    ppv = precision_score(y_test, y_pred)  # Positive Predictive Value (PPV)\n",
    "    npv = tn / (tn + fn)  # Negative Predictive Value (NPV)\n",
    "    f1 = f1_score(y_test, y_pred)  # F1 Score\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)  # Matthews Correlation Coefficient (MCC)\n",
    "    youden_index = sensitivity + specificity - 1  # Youden's Index\n",
    "    utility_score = (sensitivity + specificity) / 2  # Utility Score (average of sensitivity and specificity)\n",
    "    \n",
    "    # Use bootstrapping to calculate the 95% confidence interval for AUC\n",
    "    n_bootstraps = 1000\n",
    "    auc_scores = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Bootstrap resampling\n",
    "        y_test_resampled, y_score_resampled = resample(y_test, y_score, random_state=np.random.randint(1, 100))\n",
    "        auc_resampled = roc_auc_score(y_test_resampled, y_score_resampled)\n",
    "        auc_scores.append(auc_resampled)\n",
    "    \n",
    "    # Calculate 95% confidence interval for AUC\n",
    "    auc_scores_sorted = np.array(auc_scores)\n",
    "    auc_scores_sorted.sort()\n",
    "    lower_bound = auc_scores_sorted[int(0.025 * len(auc_scores_sorted))]  # 2.5th percentile\n",
    "    upper_bound = auc_scores_sorted[int(0.975 * len(auc_scores_sorted))]  # 97.5th percentile\n",
    "    \n",
    "    # Plot ROC curve for the current model\n",
    "    plt.plot(fpr, tpr, label=f'{model.__class__.__name__} (AUC = {auc:.3f} [{lower_bound:.3f}-{upper_bound:.3f}])')\n",
    "\n",
    "    # Store results for each model in a list\n",
    "    results.append({\n",
    "        'Model': model.__class__.__name__,\n",
    "        'AUC': auc,\n",
    "        'Accuracy': accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'PPV': ppv,\n",
    "        'NPV': npv,\n",
    "        'F1 Score': f1,\n",
    "        'MCC': mcc,\n",
    "        'Youden Index': youden_index,\n",
    "        'Utility Score': utility_score\n",
    "    })\n",
    "\n",
    "# Plot a diagonal line representing random guessing (no predictive power)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.01])\n",
    "\n",
    "# Add plot labels and title\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve', fontsize=15)\n",
    "\n",
    "# Add a legend to the lower-right corner of the plot\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Remove the top and right plot boundaries for cleaner look\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "plt.savefig('test_ROC.svg', format='svg')\n",
    "\n",
    "# Display the ROC plot\n",
    "plt.show()\n",
    "\n",
    "# Convert the results list into a DataFrame and print it\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results DataFrame as a CSV file\n",
    "results_df.to_csv('test_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Replace the column names in the plot with cleaned names (using a name mapping)\n",
    "cleaned_feature_names = [name_mapping.get(name, name) for name in X_test.columns]\n",
    "\n",
    "# Create a SHAP explainer for the random forest model\n",
    "explainer = shap.Explainer(rf.predict, X)\n",
    "\n",
    "# Randomly select 800 samples from the test set\n",
    "X_test_sampled = X_test.sample(n=800, random_state=42)\n",
    "\n",
    "# Calculate SHAP values for the selected samples\n",
    "shap_values = explainer(X_test_sampled)\n",
    "\n",
    "# Create a SHAP summary plot with the SHAP values, using the cleaned feature names\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_test_sampled, feature_names=cleaned_feature_names, max_display=10, show=False)\n",
    "\n",
    "# Save the plot as an SVG file\n",
    "plt.savefig('shap.svg', format='svg')\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
